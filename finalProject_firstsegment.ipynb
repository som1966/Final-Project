{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finalProject-firstsegment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSnTLy88idGNMfN0fkLLJo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/som1966/Final-Project/blob/zohairBranch/finalProject_firstsegment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reading in files from data storage so that we can manipulate them"
      ],
      "metadata": {
        "id": "nMf1P6QXlIWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-3.0.3'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UweyFzltlK64",
        "outputId": "d3009c0f-1bd6-4d8f-f523-ff888126a210"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Wa\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 3s (95.3 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, enter the following code to download a Postgres driver that will allow Spark to interact with Postgres:"
      ],
      "metadata": {
        "id": "Ucez_BK0m827"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30WnMu9mnALd",
        "outputId": "b709c312-535b-40bc-c1cd-42823e6b2b71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-17 13:34:00--  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1002883 (979K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.16.jar.12’\n",
            "\n",
            "postgresql-42.2.16. 100%[===================>] 979.38K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-01-17 13:34:01 (6.56 MB/s) - ‘postgresql-42.2.16.jar.12’ saved [1002883/1002883]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"FinalProject-ETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
      ],
      "metadata": {
        "id": "wVHAU-NXnF42"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "performed the  first two steps of the ETL process before pyspark"
      ],
      "metadata": {
        "id": "BP05mXWPnlVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to data storage --- Extract \n",
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url1 =\"https://corti-gw-group-project.s3.amazonaws.com/Unprocessed+Data/Happiness_Scores_2019.csv\"\n",
        "spark.sparkContext.addFile(url1)\n",
        "Happiness_Scores_2019_df = spark.read.csv(SparkFiles.get(\"Happiness_Scores_2019.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "Happiness_Scores_2019_df = Happiness_Scores_2019_df.drop('yr') \\\n",
        "       .drop('country_name')\n",
        "\n",
        "Happiness_Scores_2019_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlgYq5jNnt6t",
        "outputId": "6f2716dd-46cc-493c-a7d3-0d16c242a075"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+\n",
            "|country_code|Life Ladder|\n",
            "+------------+-----------+\n",
            "|           4|      2.375|\n",
            "|           8|      4.995|\n",
            "|          12|      4.745|\n",
            "|          32|      6.086|\n",
            "|          51|      5.488|\n",
            "|          36|      7.234|\n",
            "|          40|      7.195|\n",
            "|          31|      5.173|\n",
            "|          48|      7.098|\n",
            "|          50|      5.114|\n",
            "|         112|      5.821|\n",
            "|          56|      6.772|\n",
            "|         204|      4.976|\n",
            "|          68|      5.674|\n",
            "|          70|      6.016|\n",
            "|          72|      3.471|\n",
            "|          76|      6.451|\n",
            "|         100|      5.108|\n",
            "|         854|      4.741|\n",
            "|         116|      4.998|\n",
            "+------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "url2 =\"https://corti-gw-group-project.s3.amazonaws.com/Unprocessed+Data/UN_Consumer_Price+Index_2019.csv\"\n",
        "spark.sparkContext.addFile(url2)\n",
        "UN_Consumer_Price_Index_2019_df = spark.read.csv(SparkFiles.get(\"UN_Consumer_Price+Index_2019.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "#Remove multiple columnss in one go\n",
        "UN_Consumer_Price_Index_2019_df = UN_Consumer_Price_Index_2019_df.drop('Year')\n",
        "UN_Consumer_Price_Index_2019_df = UN_Consumer_Price_Index_2019_df.drop('_c1')\\\n",
        "        .drop('series')\n",
        "UN_Consumer_Price_Index_2019_df=UN_Consumer_Price_Index_2019_df.withColumnRenamed(\"Value\",\"consumer_price_index\")\n",
        "\n",
        "UN_Consumer_Price_Index_2019_df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv7WXkmMpW4u",
        "outputId": "c5d51372-e139-48a7-ba0c-a96d562e46aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------------------+\n",
            "|Region/Country/Area|consumer_price_index|\n",
            "+-------------------+--------------------+\n",
            "|4                  |149.9               |\n",
            "|8                  |119.1               |\n",
            "|12                 |151.4               |\n",
            "|20                 |105.8               |\n",
            "|24                 |378.9               |\n",
            "|660                |107.1               |\n",
            "|28                 |115.4               |\n",
            "|32                 |232.8               |\n",
            "|51                 |129.2               |\n",
            "|533                |109.2               |\n",
            "|36                 |119.8               |\n",
            "|40                 |118.1               |\n",
            "|31                 |156.9               |\n",
            "|44                 |116.2               |\n",
            "|48                 |118.8               |\n",
            "|50                 |179.7               |\n",
            "|52                 |191.7               |\n",
            "|112                |508.1               |\n",
            "|56                 |117.1               |\n",
            "|84                 |106.2               |\n",
            "+-------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url3 =\"https://corti-gw-group-project.s3.amazonaws.com/Unprocessed+Data/UN_Country.csv\"\n",
        "spark.sparkContext.addFile(url3)\n",
        "UN_Country_df = spark.read.csv(SparkFiles.get(\"UN_Country.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "UN_Country_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwFdrT7dq-UT",
        "outputId": "055a053e-fb78-457e-e4df-bf975357d4ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------------+\n",
            "|country_code|       country_name|\n",
            "+------------+-------------------+\n",
            "|           4|        Afghanistan|\n",
            "|           8|            Albania|\n",
            "|          12|            Algeria|\n",
            "|          16|     American Samoa|\n",
            "|          20|            Andorra|\n",
            "|          24|             Angola|\n",
            "|         660|           Anguilla|\n",
            "|          28|Antigua and Barbuda|\n",
            "|          32|          Argentina|\n",
            "|          51|            Armenia|\n",
            "|         533|              Aruba|\n",
            "|          36|          Australia|\n",
            "|          40|            Austria|\n",
            "|          31|         Azerbaijan|\n",
            "|          44|            Bahamas|\n",
            "|          48|            Bahrain|\n",
            "|          50|         Bangladesh|\n",
            "|          52|           Barbados|\n",
            "|         112|            Belarus|\n",
            "|          56|            Belgium|\n",
            "+------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url4 =\"https://corti-gw-group-project.s3.amazonaws.com/Unprocessed+Data/UN_GDP_Per_Capita_2019.csv\"\n",
        "spark.sparkContext.addFile(url4)\n",
        "UN_GDP_Per_Capita_2019_df = spark.read.csv(SparkFiles.get(\"UN_GDP_Per_Capita_2019.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "#Remove multiple columnss in one go\n",
        "UN_GDP_Per_Capita_2019_df = UN_GDP_Per_Capita_2019_df.drop('Year')\n",
        "UN_GDP_Per_Capita_2019_df = UN_GDP_Per_Capita_2019_df.drop('_c1')\\\n",
        "        .drop('series')\n",
        "UN_GDP_Per_Capita_2019_df=UN_GDP_Per_Capita_2019_df.withColumnRenamed(\"Value\",\"per_capita_gdp_dollars\")\n",
        "\n",
        "UN_GDP_Per_Capita_2019_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO2C7UFRrU4f",
        "outputId": "1512df82-265e-491d-8180-232a5ae070ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------------------+\n",
            "|Region/Country/Area|per_capita_gdp_dollars|\n",
            "+-------------------+----------------------+\n",
            "|4                  |$470.00               |\n",
            "|8                  |$5,303.00             |\n",
            "|12                 |$3,976.00             |\n",
            "|20                 |$40,887.00            |\n",
            "|24                 |$2,671.00             |\n",
            "|660                |$25,529.00            |\n",
            "|28                 |$17,113.00            |\n",
            "|32                 |$10,041.00            |\n",
            "|51                 |$4,623.00             |\n",
            "|533                |$30,975.00            |\n",
            "|36                 |$54,763.00            |\n",
            "|40                 |$49,701.00            |\n",
            "|31                 |$4,782.00             |\n",
            "|44                 |$34,864.00            |\n",
            "|48                 |$23,504.00            |\n",
            "|50                 |$1,846.00             |\n",
            "|52                 |$18,149.00            |\n",
            "|112                |$6,674.00             |\n",
            "|56                 |$46,198.00            |\n",
            "|84                 |$4,815.00             |\n",
            "+-------------------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url5 =\"https://corti-gw-group-project.s3.amazonaws.com/Unprocessed+Data/UN_Gender_Ratio_2019.csv\"\n",
        "spark.sparkContext.addFile(url5)\n",
        "UN_Gender_Ratio_2019_df = spark.read.csv(SparkFiles.get(\"UN_Gender_Ratio_2019.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "#Remove multiple columnss in one go\n",
        "UN_Gender_Ratio_2019_df = UN_Gender_Ratio_2019_df.drop('Year')\n",
        "UN_Gender_Ratio_2019_df = UN_Gender_Ratio_2019_df.drop('_c1')\\\n",
        "        .drop('series')\n",
        "UN_Gender_Ratio_2019_df=UN_Gender_Ratio_2019_df.withColumnRenamed(\"Value\",\"gender_ratio_males_per100_female\")\n",
        "\n",
        "UN_Gender_Ratio_2019_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cde480e-5855-490d-b2cf-2ce55066a368",
        "id": "3o27KOh4r0n8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------------------------------+\n",
            "|Region/Country/Area|gender_ratio_males_per100_female|\n",
            "+-------------------+--------------------------------+\n",
            "|4                  |105.5                           |\n",
            "|8                  |103.7                           |\n",
            "|12                 |102.1                           |\n",
            "|24                 |97.9                            |\n",
            "|28                 |93.2                            |\n",
            "|32                 |95.2                            |\n",
            "|51                 |88.8                            |\n",
            "|533                |90.3                            |\n",
            "|36                 |99.2                            |\n",
            "|40                 |97.0                            |\n",
            "|31                 |99.7                            |\n",
            "|44                 |94.5                            |\n",
            "|48                 |179.9                           |\n",
            "|50                 |102.4                           |\n",
            "|52                 |93.7                            |\n",
            "|112                |87.1                            |\n",
            "|56                 |98.0                            |\n",
            "|84                 |99.1                            |\n",
            "|204                |99.7                            |\n",
            "|64                 |113.1                           |\n",
            "+-------------------+--------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url6 =\"https://corti-gw-group-project.s3.amazonaws.com/Unprocessed+Data/UN_Infant_Mortality_2020.csv\"\n",
        "spark.sparkContext.addFile(url6)\n",
        "UN_Infant_Mortality_2020_df = spark.read.csv(SparkFiles.get(\"UN_Infant_Mortality_2020.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "#Remove multiple columnss in one go\n",
        "UN_Infant_Mortality_2020_df = UN_Infant_Mortality_2020_df.drop('Year')\n",
        "UN_Infant_Mortality_2020_df = UN_Infant_Mortality_2020_df.drop('_c1')\\\n",
        "        .drop('series')\n",
        "UN_Infant_Mortality_2020_df=UN_Infant_Mortality_2020_df.withColumnRenamed(\"Value\",\"infant_mortality_per1000_births\")\n",
        "UN_Infant_Mortality_2020_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfe_W1Eirkxj",
        "outputId": "5f3f1fd2-4819-418f-c773-66fb680469bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------------------+\n",
            "|Region/Country/Area|infant_mortality_per1000_births|\n",
            "+-------------------+-------------------------------+\n",
            "|4                  |51.7                           |\n",
            "|8                  |8.0                            |\n",
            "|12                 |21.2                           |\n",
            "|24                 |61.5                           |\n",
            "|28                 |5.2                            |\n",
            "|32                 |10.2                           |\n",
            "|51                 |10.8                           |\n",
            "|533                |13.6                           |\n",
            "|36                 |3.1                            |\n",
            "|40                 |3.2                            |\n",
            "|31                 |20.8                           |\n",
            "|44                 |5.9                            |\n",
            "|48                 |6.0                            |\n",
            "|50                 |26.8                           |\n",
            "|52                 |10.0                           |\n",
            "|112                |3.0                            |\n",
            "|56                 |2.8                            |\n",
            "|84                 |12.8                           |\n",
            "|204                |61.1                           |\n",
            "|64                 |24.1                           |\n",
            "+-------------------+-------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url7 =\"https://corti-gw-group-project.s3.amazonaws.com/Unprocessed+Data/UN_Life_Expectancy_2020.csv\"\n",
        "spark.sparkContext.addFile(url7)\n",
        "UN_Life_Expectancy_2020_df = spark.read.csv(SparkFiles.get(\"UN_Life_Expectancy_2020.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "UN_Life_Expectancy_2020_df = UN_Life_Expectancy_2020_df.drop('Year')\n",
        "UN_Life_Expectancy_2020_df = UN_Life_Expectancy_2020_df.drop('_c1')\\\n",
        "        .drop('series')\n",
        "UN_Life_Expectancy_2020_df=UN_Life_Expectancy_2020_df.withColumnRenamed(\"Value\",\"Life_Expectancy\")\n",
        "\n",
        "UN_Life_Expectancy_2020_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjL3F-oZsXpT",
        "outputId": "4b585994-cc52-4b62-f3f6-7856c24583b8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------------+\n",
            "|Region/Country/Area|Life_Expectancy|\n",
            "+-------------------+---------------+\n",
            "|                  4|           64.3|\n",
            "|                  8|           78.4|\n",
            "|                 12|           76.6|\n",
            "|                 24|           60.5|\n",
            "|                 28|           76.8|\n",
            "|                 32|           76.4|\n",
            "|                 51|           74.9|\n",
            "|                533|           76.1|\n",
            "|                 36|           83.2|\n",
            "|                 40|           81.4|\n",
            "|                 31|           72.8|\n",
            "|                 44|           73.7|\n",
            "|                 48|           77.1|\n",
            "|                 50|           72.2|\n",
            "|                 52|           79.0|\n",
            "|                112|           74.5|\n",
            "|                 56|           81.4|\n",
            "|                 84|           74.4|\n",
            "|                204|           61.3|\n",
            "|                 64|           71.3|\n",
            "+-------------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url8 =\"https://corti-gw-group-project.s3.amazonaws.com/Unprocessed+Data/UN_Population_Density_2019.csv\"\n",
        "spark.sparkContext.addFile(url8)\n",
        "table8_df = spark.read.csv(SparkFiles.get(\"UN_Population_Density_2019.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "table8_df = table8_df.drop('Year')\n",
        "table8_df = table8_df.drop('_c1')\\\n",
        "        .drop('series')\n",
        "table8_df=table8_df.withColumnRenamed(\"Value\",\"population_density\")\n",
        "\n",
        "table8_df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xQ-sLGpu3C8",
        "outputId": "f9524a18-b1c4-49cd-815a-392d004ce721"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------------+\n",
            "|Region/Country/Area|population_density|\n",
            "+-------------------+------------------+\n",
            "|4                  |58.3              |\n",
            "|8                  |105.1             |\n",
            "|12                 |18.1              |\n",
            "|16                 |276.6             |\n",
            "|20                 |164.1             |\n",
            "|24                 |25.5              |\n",
            "|660                |165.2             |\n",
            "|28                 |220.7             |\n",
            "|32                 |16.4              |\n",
            "|51                 |103.9             |\n",
            "|533                |590.6             |\n",
            "|36                 |3.3               |\n",
            "|40                 |108.7             |\n",
            "|31                 |121.6             |\n",
            "|44                 |38.9              |\n",
            "|48                 |2159.4            |\n",
            "|50                 |1252.6            |\n",
            "|52                 |667.5             |\n",
            "|112                |46.6              |\n",
            "|56                 |381.1             |\n",
            "+-------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url9 =\"https://corti-gw-group-project.s3.amazonaws.com/Unprocessed+Data/UN_Seats_Held_by_Women_2019.csv\"\n",
        "spark.sparkContext.addFile(url9)\n",
        "table9_df = spark.read.csv(SparkFiles.get(\"UN_Seats_Held_by_Women_2019.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "table9_df = table9_df.drop('series')\\\n",
        "        .drop('Year')\\\n",
        "        .drop('_c1')\\\n",
        "        .drop('Last Election Date')\\\n",
        "        .drop('Last Election Date footnote')\n",
        "table9_df=table9_df.withColumnRenamed(\"Value\",\"seats_held_by_woman_pct\")\n",
        "\n",
        "table9_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PErp6qYSx61i",
        "outputId": "e2ed76cc-3e11-43cd-c5d1-a788f6733c05"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------------------+\n",
            "|Region/Country/Area|seats_held_by_woman_pct|\n",
            "+-------------------+-----------------------+\n",
            "|8                  |29.3                   |\n",
            "|12                 |25.8                   |\n",
            "|20                 |32.1                   |\n",
            "|24                 |30.0                   |\n",
            "|28                 |11.1                   |\n",
            "|32                 |38.8                   |\n",
            "|51                 |24.2                   |\n",
            "|36                 |30.0                   |\n",
            "|40                 |37.2                   |\n",
            "|31                 |16.8                   |\n",
            "|44                 |12.8                   |\n",
            "|48                 |15.0                   |\n",
            "|50                 |20.7                   |\n",
            "|52                 |20.0                   |\n",
            "|112                |34.5                   |\n",
            "|56                 |38.0                   |\n",
            "|84                 |9.4                    |\n",
            "|204                |7.2                    |\n",
            "|64                 |14.9                   |\n",
            "|68                 |53.1                   |\n",
            "+-------------------+-----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url10 =\"https://corti-gw-group-project.s3.amazonaws.com/Unprocessed+Data/UN_Unemployment_Rate_2020.csv\"\n",
        "spark.sparkContext.addFile(url10)\n",
        "table10_df = spark.read.csv(SparkFiles.get(\"UN_Unemployment_Rate_2020.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "table10_df = table10_df.drop('_c1')\\\n",
        "        .drop('series')\\\n",
        "        .drop('Last Election Date')\\\n",
        "        .drop('Year')\\\n",
        "        .drop('Last Election Date footnote')\n",
        "table10_df=table10_df.withColumnRenamed(\"Value\",\"unemployment_rate\")\n",
        "\n",
        "table10_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYMwmKtRzGat",
        "outputId": "39a00623-4a34-4441-ab13-b9973d69aeec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------------+\n",
            "|Region/Country/Area|unemployment_rate|\n",
            "+-------------------+-----------------+\n",
            "|4                  |11.2             |\n",
            "|8                  |12.8             |\n",
            "|12                 |11.5             |\n",
            "|24                 |6.8              |\n",
            "|32                 |10.4             |\n",
            "|51                 |16.6             |\n",
            "|36                 |5.3              |\n",
            "|40                 |4.8              |\n",
            "|31                 |6.0              |\n",
            "|44                 |11.3             |\n",
            "|48                 |0.8              |\n",
            "|50                 |4.2              |\n",
            "|52                 |10.9             |\n",
            "|112                |4.6              |\n",
            "|56                 |5.7              |\n",
            "|84                 |6.4              |\n",
            "|204                |2.0              |\n",
            "|64                 |2.4              |\n",
            "|68                 |3.5              |\n",
            "|70                 |18.4             |\n",
            "+-------------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url11 =\"https://corti-gw-group-project.s3.amazonaws.com/Unprocessed+Data/UN_Water_Services_2020.csv\"\n",
        "spark.sparkContext.addFile(url11)\n",
        "table11_df = spark.read.csv(SparkFiles.get(\"UN_Water_Services_2020.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "table11_df = table11_df.drop('_c1')\\\n",
        "        .drop('series')\\\n",
        "        .drop('Last Election Date')\\\n",
        "        .drop('Year')\\\n",
        "        .drop('Last Election Date footnote')\n",
        "table11_df=table11_df.withColumnRenamed(\"Value\",\"safe_drinking_water_access_pct\")\n",
        "\n",
        "table11_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C6eYzw3zhQh",
        "outputId": "76f2cfc4-e794-4340-ae24-c2ceec30f9c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------------------------+\n",
            "|Region/Country/Area|safe_drinking_water_access_pct|\n",
            "+-------------------+------------------------------+\n",
            "|4                  |27.6                          |\n",
            "|8                  |70.7                          |\n",
            "|12                 |72.4                          |\n",
            "|16                 |98.4                          |\n",
            "|20                 |90.6                          |\n",
            "|51                 |86.9                          |\n",
            "|40                 |98.9                          |\n",
            "|31                 |88.3                          |\n",
            "|48                 |99.0                          |\n",
            "|50                 |58.5                          |\n",
            "|112                |94.6                          |\n",
            "|56                 |99.9                          |\n",
            "|64                 |36.6                          |\n",
            "|70                 |88.9                          |\n",
            "|76                 |85.8                          |\n",
            "|100                |97.6                          |\n",
            "|116                |27.8                          |\n",
            "|124                |99.0                          |\n",
            "|140                |6.2                           |\n",
            "|148                |5.6                           |\n",
            "+-------------------+------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url12 =\"https://zohair-gw-group-project.s3.amazonaws.com/Unprocessed+Data/Democracy_Index_2019.csv\"\n",
        "spark.sparkContext.addFile(url12)\n",
        "table12_df = spark.read.csv(SparkFiles.get(\"Democracy_Index_2019.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "table12_df = table12_df.drop('country_name')\\\n",
        "        .drop('yr')\n",
        "table12_df=table12_df.withColumnRenamed(\"Democracy index (EIU)\",\"democracy_index\")\n",
        "\n",
        "table12_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "Ww7DhWtB1WGB",
        "outputId": "755e7dd6-65fc-4432-8b39-ae83d0f87efd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0a256c7cf2dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl12\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"https://zohair-gw-group-project.s3.amazonaws.com/Unprocessed+Data/Democracy_Index_2019.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtable12_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSparkFiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Democracy_Index_2019.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtable12_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable12_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'country_name'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/tmp/spark-efaa5bfc-9197-4f67-8eae-167410f7fb4e/userFiles-b8677eb9-67b9-48b5-81d8-399770c574a7/Democracy_Index_2019.csv;"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# joined the two dataframes\n",
        "joined1_df= UN_Country_df.join(Happiness_Scores_2019_df, on=\"country_code\", how=\"inner\")\n",
        "joined1_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FAzLS5P2NyOw",
        "outputId": "87e14efc-ab41-4c84-c3fd-99c5539c3de2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-20b3348925e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# joined the two dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mjoined1_df\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mUN_Country_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHappiness_Scores_2019_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"country_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mjoined1_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o213.showString.\n: java.util.concurrent.ExecutionException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 37, 1b2df0af1e31, executor driver): java.io.IOException: Server returned HTTP response code: 403 for URL: https://zohair-gw-group-project.s3.amazonaws.com/Unprocessed+Data/Democracy_Index_2019.csv\n\tat java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1924)\n\tat java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1520)\n\tat java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:250)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:729)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:535)\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$4(Executor.scala:876)\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$4$adapted(Executor.scala:872)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:872)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:423)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.base/java.util.concurrent.FutureTask.get(FutureTask.java:205)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:195)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:515)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:193)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:116)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:210)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:100)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:71)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:97)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:222)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:137)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:97)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:51)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:95)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:51)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:632)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:692)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 37, 1b2df0af1e31, executor driver): java.io.IOException: Server returned HTTP response code: 403 for URL: https://zohair-gw-group-project.s3.amazonaws.com/Unprocessed+Data/Democracy_Index_2019.csv\n\tat java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1924)\n\tat java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1520)\n\tat java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:250)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:729)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:535)\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$4(Executor.scala:876)\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$4$adapted(Executor.scala:872)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:872)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:423)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2179)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:397)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:182)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.io.IOException: Server returned HTTP response code: 403 for URL: https://zohair-gw-group-project.s3.amazonaws.com/Unprocessed+Data/Democracy_Index_2019.csv\n\tat java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1924)\n\tat java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1520)\n\tat java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:250)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:729)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:535)\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$4(Executor.scala:876)\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$4$adapted(Executor.scala:872)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:872)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:423)\n\t... 3 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform\n"
      ],
      "metadata": {
        "id": "f1sJp86JsoMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/\n",
        "\n",
        "https://sparkbyexamples.com/pyspark/pyspark-rename-dataframe-column/\n",
        "\n",
        "\n",
        "https://jacobcelestine.com/knowledge_repo/colab_and_pyspark/\n",
        "\n"
      ],
      "metadata": {
        "id": "IoNL07SMEH9J"
      }
    }
  ]
}